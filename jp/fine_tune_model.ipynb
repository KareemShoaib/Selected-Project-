{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Comprehensive text cleaning for product search\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove special characters but keep important ones for products\n",
    "        text = re.sub(r'[^\\w\\s\\-\\.]', ' ', text)\n",
    "        \n",
    "        # Handle measurements and units (keep them meaningful)\n",
    "        text = re.sub(r'(\\d+)\\s*-\\s*(\\w+)', r'\\1\\2', text)  # \"12-gauge\" -> \"12gauge\"\n",
    "        text = re.sub(r'(\\d+)\\s*(\\w+)', r'\\1\\2', text)      # \"1 gal\" -> \"1gal\"\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Extract meaningful features from product text\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        features = []\n",
    "        \n",
    "        # Extract brand information (first few words often contain brand)\n",
    "        words = text.split()\n",
    "        if len(words) > 0:\n",
    "            potential_brand = words[0]\n",
    "            features.append(f\"brand_{potential_brand}\")\n",
    "        \n",
    "        # Extract measurements\n",
    "        measurements = re.findall(r'\\d+(?:\\.\\d+)?(?:inch|in|ft|gal|gauge|lb)', text)\n",
    "        features.extend([f\"measure_{m}\" for m in measurements])\n",
    "        \n",
    "        colors = re.findall(r'\\b(?:black|white|brown|gray|grey|red|blue|green|yellow|silver|gold)\\b', text)\n",
    "        features.extend([f\"color_{c}\" for c in colors])\n",
    "        \n",
    "        return \" \".join(features)\n",
    "    \n",
    "    def remove_stopwords_and_stem(self, text):\n",
    "        \"\"\"Remove stopwords and apply stemming\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        words = text.split()\n",
    "        important_words = {'with', 'for', 'in', 'on', 'over', 'under'}\n",
    "        filtered_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in self.stop_words or word in important_words:\n",
    "                stemmed = self.stemmer.stem(word)\n",
    "                filtered_words.append(stemmed)\n",
    "        \n",
    "        return \" \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/small/train.csv')\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna(subset=['product_title', 'search_term', 'relevance'])\n",
    "print(f\"After removing duplicates and nulls: {df.shape}\")\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "df['search_term_clean'] = df['search_term'].apply(preprocessor.clean_text)\n",
    "df['product_title_clean'] = df['product_title'].apply(preprocessor.clean_text)\n",
    "\n",
    "df['search_term_features'] = df['search_term'].apply(preprocessor.extract_features)\n",
    "df['product_title_features'] = df['product_title'].apply(preprocessor.extract_features)\n",
    "\n",
    "df['search_term'] = df['search_term_clean'] + \" \" + df['search_term_features']\n",
    "df['product_title'] = df['product_title_clean'] + \" \" + df['product_title_features']\n",
    "df = df.drop(columns=['search_term_clean', 'product_title_clean', 'search_term_features', 'product_title_features'])\n",
    "\n",
    "df['search_term'] = df['search_term'].apply(preprocessor.remove_stopwords_and_stem)\n",
    "df['product_title'] = df['product_title'].apply(preprocessor.remove_stopwords_and_stem)\n",
    "\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['relevance'].quantile(0.25)\n",
    "Q3 = df['relevance'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Outlier bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "outliers_count = len(df[(df['relevance'] < lower_bound) | (df['relevance'] > upper_bound)])\n",
    "print(f\"Number of outliers: {outliers_count}\")\n",
    "\n",
    "df['relevance'] = df['relevance'].clip(lower_bound, upper_bound)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['relevance'] = scaler.fit_transform(df[['relevance']])\n",
    "\n",
    "print(\"\\nProcessed relevance distribution:\")\n",
    "print(df['relevance'].describe())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training/Validation Split with Stratification\n",
    "# Stratify based on relevance bins to ensure balanced splits\n",
    "y_stratify = pd.cut(df_augmented['relevance'], bins=5, labels=False)\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    df_augmented, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_stratify\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "\n",
    "# Ensure no data leakage by checking product_uid overlap\n",
    "train_products = set(train_data.get('product_uid', []))\n",
    "val_products = set(val_data.get('product_uid', []))\n",
    "if 'product_uid' in df_augmented.columns:\n",
    "    overlap = len(train_products.intersection(val_products))\n",
    "    print(f\"Product overlap between train/val: {overlap} products\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_final, test_size=0.2, random_state=42)\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=[s, p], label=float(r))\n",
    "    for s, p, r in zip(train_df['search_term_enhanced'], train_df['product_title_enhanced'], train_df['relevance_normalized'])\n",
    "]\n",
    "\n",
    "val_examples = [\n",
    "    InputExample(texts=[s, p], label=float(r))\n",
    "    for s, p, r in zip(val_df['search_term_enhanced'], val_df['product_title_enhanced'], val_df['relevance_normalized'])\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "val_dataloader = DataLoader(val_examples, shuffle=False, batch_size=16)\n",
    "\n",
    "val_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name='val')\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=val_evaluator,\n",
    "    epochs=3,\n",
    "    warmup_steps=100,\n",
    "    evaluation_steps=100,\n",
    "    show_progress_bar=True,\n",
    "    output_path='fine_tuned_model'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
